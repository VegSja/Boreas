# DLT Pipelines - Data Ingestion

Data Load Tool (DLT) pipelines for ingesting avalanche and weather data from external APIs into DuckDB.

## Architecture

The DLT component handles the **Extract** and **Load** phases of the ELT pipeline:
- **Extract**: Fetch data from external APIs
- **Transform**: Minimal data normalization and validation
- **Load**: Store raw data in DuckDB (Bronze layer)

## Quick Start

### Prerequisites
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) package manager
- DLT >= 1.20.0
- DuckDB >= 1.4.3
- Valid API credentials (configured in `config.toml`)


### Running Pipelines

```bash
# Run all pipelines
uv run dlt/run_dlt_pipelines.py
```

## ðŸ”§ Pipeline Configuration

### Pipeline Definitions (`pipelines/`)

**avalanche_pipeline.py**:
```python
def create_avalanche_pipeline():
    pipeline = dlt.pipeline(
        pipeline_name="avalanche_pipeline",
        destination=dlt.destinations.duckdb(
            destination_name=db_path,
            enable_dataset_name_normalization=False
        ),
        dataset_name="1_bronze",
        progress='enlighten'
    )
    return pipeline
```

**Features**:
- **Destination**: DuckDB database
- **Schema**: `1_bronze` (bronze layer)
- **Progress**: Visual progress bars
- **State Management**: Automatic incremental loading

## File Structure

```
dlt/
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ avalanche_pipeline.py    # Avalanche data pipeline
â”‚   â”œâ”€â”€ weather_pipeline.py      # Weather data pipeline
â”‚   â””â”€â”€ region_pipeline.py       # Region metadata pipeline
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ avalanche/
â”‚   â”‚   â”œâ”€â”€ avalanche_warnings.py
â”‚   â”‚   â””â”€â”€ avalanche_helper.py
â”‚   â”œâ”€â”€ weather/
â”‚   â”‚   â”œâ”€â”€ weather_forecast.py
â”‚   â”‚   â”œâ”€â”€ weather_historic.py
â”‚   â”‚   â””â”€â”€ weather_common.py
â”‚   â””â”€â”€ regions/
â”‚       â””â”€â”€ region_source.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logging.py               # Logging configuration
â”œâ”€â”€ .dlt/
â”‚   â””â”€â”€ config.toml             # DLT configuration
â”œâ”€â”€ exceptions.py               # Custom exceptions
â””â”€â”€ run_dlt_pipelines.py        # Main execution script
```

## Data Quality & Validation

### Logging
- Structured logging with JSON format
- API request/response tracking
- Pipeline execution metrics
- Error reporting and alerting

## ðŸ”§ Monitoring & Maintenance

### Performance Monitoring
```bash
# Check pipeline state
uv run dlt pipeline avalanche_pipeline info

# View pipeline metrics
uv run dlt pipeline avalanche_pipeline trace

# Reset pipeline state (if needed)
uv run dlt pipeline avalanche_pipeline drop
```

## ðŸ”— Integration

### Upstream Dependencies
- External API availability
- Network connectivity
- Valid authentication credentials

### Downstream Integration
- Outputs to DuckDB (`../boreas.duckdb`)
- Consumed by dbt transformations
- Bronze layer tables for analytics

## ðŸ“š Resources

- [DLT Documentation](https://dlthub.com/docs)
- [DuckDB Integration](https://dlthub.com/docs/dlt-ecosystem/destinations/duckdb)
- [Source Development Guide](https://dlthub.com/docs/walkthroughs/create-a-pipeline)